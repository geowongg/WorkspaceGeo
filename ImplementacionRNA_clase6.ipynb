{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D6yuS7qywBp"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np \n",
        "\n",
        "class Network(object): \n",
        "  def __init__(self,sizes):\n",
        "     self.num_layers = len(sizes)\n",
        "     self.sizes = sizes\n",
        "     self.biases = [np.random.randn(y, 1) for y in sizes[1:]] #matriz de una sola columna que nos dará los datos a entrenar \n",
        "     self.weights = [np.random.randn(y, x)\n",
        "                    for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "  def feedforward(self, a):  #esta función es para eveluar la red neuronal \n",
        "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "  def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"Train the neural network using mini-batch stochastic\n",
        "        gradient descent.  The ``training_data`` is a list of tuples\n",
        "        ``(x, y)`` representing the training inputs and the desired\n",
        "        outputs.  The other non-optional parameters are\n",
        "        self-explanatory.  If ``test_data`` is provided then the\n",
        "        network will be evaluated against the test data after each\n",
        "        epoch, and partial progress printed out.  This is useful for\n",
        "        tracking progress, but slows things down substantially.\"\"\"\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Update the network's weights and biases by applying\n",
        "        gradient descent using backpropagation to a single mini batch.\n",
        "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
        "        is the learning rate.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "  def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
        "        gradient for the cost function C_x.  ``nabla_b`` and\n",
        "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
        "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "  def evaluate(self, test_data):\n",
        "        \"\"\"Return the number of test inputs for which the neural\n",
        "        network outputs the correct result. Note that the neural\n",
        "        network's output is assumed to be the index of whichever\n",
        "        neuron in the final layer has the highest activation.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
        "        \\partial a for the output activations.\"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.tutorialspoint.com/keras/keras_installation.htm\n",
        "#https://docs.python.org/es/3/tutorial/venv.html\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD\n",
        "\n",
        "dataset=mnist.load_data()\n",
        "#print(len(dataset))\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = dataset\n",
        "#print(y_train.shape)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "#x_train=x_train[0:8000]\n",
        "#x_test=x_train[0:1000]\n",
        "#y_train=y_train[0:8000]\n",
        "#y_test=y_train[0:1000]\n",
        "x_trainv = x_train.reshape(60000, 784)\n",
        "x_testv = x_test.reshape(10000, 784)\n",
        "#print(x_trainv[3])\n",
        "x_trainv = x_trainv.astype('float32')\n",
        "x_testv = x_testv.astype('float32')\n",
        "x_trainv /= 255  # x_trainv = x_trainv/255\n",
        "x_testv /= 255\n",
        "#print(\"linea 40--------\")\n",
        "#print(x_trainv[3])\n",
        "#print(x_train.shape)\n",
        "#print(x_trainv.shape)\n",
        "num_classes=10\n",
        "y_trainc = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_testc = keras.utils.to_categorical(y_test, num_classes)\n",
        "#print(y_trainc[6:15])\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='sigmoid', input_shape=(784,)))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n",
        "history = model.fit(x_trainv, y_trainc,\n",
        "                    batch_size=128,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_testv, y_testc))\n",
        "\n",
        "score = model.evaluate(x_testv, y_testc, verbose=1)\n",
        "print(score)\n",
        "a=model.predict(x_testv)\n",
        "#b=model.predict_proba(x_testv)\n",
        "print(a.shape)\n",
        "print(a[1])\n",
        "print(\"resultado correcto:\")\n",
        "print(y_testc[1])\n",
        "#Para guardar el modelo en disco\n",
        "model.save(\"red.h5\")\n",
        "exit()\n",
        "#para cargar la red:\n",
        "modelo_cargado = tf.keras.models.load_model('red.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMvWRUAjvB-j",
        "outputId": "dfbe3c2b-a1e9-488c-804a-c4f5a1cc5016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 1.9628 - accuracy: 0.5325 - val_loss: 1.5588 - val_accuracy: 0.7500\n",
            "Epoch 2/30\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 1.2077 - accuracy: 0.7703 - val_loss: 0.9086 - val_accuracy: 0.8149\n",
            "Epoch 3/30\n",
            "469/469 [==============================] - 6s 14ms/step - loss: 0.7735 - accuracy: 0.8314 - val_loss: 0.6462 - val_accuracy: 0.8507\n",
            "Epoch 4/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.5956 - accuracy: 0.8557 - val_loss: 0.5290 - val_accuracy: 0.8678\n",
            "Epoch 5/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.5072 - accuracy: 0.8691 - val_loss: 0.4591 - val_accuracy: 0.8806\n",
            "Epoch 6/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.4561 - accuracy: 0.8788 - val_loss: 0.4196 - val_accuracy: 0.8876\n",
            "Epoch 7/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.4231 - accuracy: 0.8831 - val_loss: 0.3936 - val_accuracy: 0.8909\n",
            "Epoch 8/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.4003 - accuracy: 0.8884 - val_loss: 0.3764 - val_accuracy: 0.8941\n",
            "Epoch 9/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3832 - accuracy: 0.8918 - val_loss: 0.3588 - val_accuracy: 0.8985\n",
            "Epoch 10/30\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.3698 - accuracy: 0.8955 - val_loss: 0.3516 - val_accuracy: 0.8988\n",
            "Epoch 11/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3591 - accuracy: 0.8971 - val_loss: 0.3401 - val_accuracy: 0.9016\n",
            "Epoch 12/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3504 - accuracy: 0.8996 - val_loss: 0.3302 - val_accuracy: 0.9044\n",
            "Epoch 13/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3429 - accuracy: 0.9012 - val_loss: 0.3239 - val_accuracy: 0.9060\n",
            "Epoch 14/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3365 - accuracy: 0.9028 - val_loss: 0.3190 - val_accuracy: 0.9079\n",
            "Epoch 15/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3306 - accuracy: 0.9046 - val_loss: 0.3179 - val_accuracy: 0.9084\n",
            "Epoch 16/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3258 - accuracy: 0.9058 - val_loss: 0.3122 - val_accuracy: 0.9114\n",
            "Epoch 17/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3212 - accuracy: 0.9071 - val_loss: 0.3054 - val_accuracy: 0.9117\n",
            "Epoch 18/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3171 - accuracy: 0.9085 - val_loss: 0.3030 - val_accuracy: 0.9126\n",
            "Epoch 19/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3137 - accuracy: 0.9089 - val_loss: 0.3024 - val_accuracy: 0.9139\n",
            "Epoch 20/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3098 - accuracy: 0.9105 - val_loss: 0.2980 - val_accuracy: 0.9154\n",
            "Epoch 21/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3065 - accuracy: 0.9116 - val_loss: 0.2955 - val_accuracy: 0.9143\n",
            "Epoch 22/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3037 - accuracy: 0.9122 - val_loss: 0.2917 - val_accuracy: 0.9162\n",
            "Epoch 23/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3012 - accuracy: 0.9130 - val_loss: 0.2890 - val_accuracy: 0.9165\n",
            "Epoch 24/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2984 - accuracy: 0.9139 - val_loss: 0.2864 - val_accuracy: 0.9176\n",
            "Epoch 25/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2955 - accuracy: 0.9141 - val_loss: 0.2864 - val_accuracy: 0.9181\n",
            "Epoch 26/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2933 - accuracy: 0.9155 - val_loss: 0.2833 - val_accuracy: 0.9174\n",
            "Epoch 27/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2910 - accuracy: 0.9153 - val_loss: 0.2859 - val_accuracy: 0.9183\n",
            "Epoch 28/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2886 - accuracy: 0.9164 - val_loss: 0.2815 - val_accuracy: 0.9201\n",
            "Epoch 29/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2865 - accuracy: 0.9172 - val_loss: 0.2778 - val_accuracy: 0.9200\n",
            "Epoch 30/30\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2845 - accuracy: 0.9183 - val_loss: 0.2766 - val_accuracy: 0.9213\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2766 - accuracy: 0.9213\n",
            "[0.2766166925430298, 0.9212999939918518]\n",
            "(10000, 10)\n",
            "[9.8357743e-01 4.3268588e-01 9.9994099e-01 9.8706508e-01 1.2239689e-04\n",
            " 9.8986554e-01 9.9626338e-01 5.3675526e-06 9.4530141e-01 2.6175380e-04]\n",
            "resultado correcto:\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}